{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from attention.img_proc.img_process import *\n",
    "from attention.img_proc.img_split import *\n",
    "from attention.models.face_models import *\n",
    "from attention.utils.img_plot import *\n",
    "from attention.utils.utilities import *\n",
    "from attention.params import *\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get attention_data directory\n",
    "current_directory = os.getcwd()\n",
    "data_directory = os.path.join(current_directory, os.pardir, \"attention_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m video_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mvideo2.MOV\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m video_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(video_folder_path, video_name)\n\u001b[0;32m----> 5\u001b[0m frames \u001b[39m=\u001b[39m extract_video_frames(video_path, \n\u001b[1;32m      6\u001b[0m                               period_sec\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m                               start_sec\u001b[39m=\u001b[39;49m\u001b[39m35\u001b[39;49m, end_sec\u001b[39m=\u001b[39;49m\u001b[39m185\u001b[39;49m)\n\u001b[1;32m      9\u001b[0m \u001b[39m# Saving the frames\u001b[39;00m\n\u001b[1;32m     10\u001b[0m image_folder_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(data_directory, \u001b[39m\"\u001b[39m\u001b[39mframes\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/Users/amine/code/aminelah0/final-attention/attention/utils/utilities.py:40\u001b[0m, in \u001b[0;36mextract_video_frames\u001b[0;34m(video_path, period_sec, start_sec, end_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39massert\u001b[39;00m end \u001b[39m>\u001b[39m start, \u001b[39m'\u001b[39m\u001b[39mSpecified start for the video capture posterior to the end\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[39mfor\u001b[39;00m period \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(start, end, period_sec \u001b[39m*\u001b[39m \u001b[39m1_000\u001b[39m):\n\u001b[0;32m---> 40\u001b[0m     video\u001b[39m.\u001b[39;49mset(cv2\u001b[39m.\u001b[39;49mCAP_PROP_POS_MSEC, period)\n\u001b[1;32m     41\u001b[0m     frame \u001b[39m=\u001b[39m video\u001b[39m.\u001b[39mread()[\u001b[39m1\u001b[39m]\n\u001b[1;32m     42\u001b[0m     image_rgb \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(frame, cv2\u001b[39m.\u001b[39mCOLOR_BGR2RGB)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Converting video to frames:\n",
    "video_folder_path = os.path.join(data_directory, \"video\")\n",
    "video_name = \"video2.MOV\"\n",
    "video_path = os.path.join(video_folder_path, video_name)\n",
    "frames = extract_video_frames(video_path, \n",
    "                              period_sec=1,\n",
    "                              start_sec=35, end_sec=185)\n",
    "\n",
    "# Saving the frames\n",
    "image_folder_path = os.path.join(data_directory, \"frames\")\n",
    "for timestamp, frame in frames.items():\n",
    "    frame_name = video_name.split('.')[0] + f'_ds{int(timestamp * 10)}'\n",
    "    save_image(frame, frame_name + '.png', image_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the face recognition model\n",
    "known_folder_path = os.path.join(data_directory, \"known_faces\")\n",
    "known_paths = load_image_paths(known_folder_path)\n",
    "known_names = list(known_paths.keys())\n",
    "known_faces = [read_image(image_path) for image_path in known_paths.values()]\n",
    "known_encodings = train_faces(known_faces, known_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS:\n",
    "n_split_w = 6\n",
    "n_split_h = 6\n",
    "landmark_idx = LEFT_EYE_EDGES  + LEFT_IRIS_CENTER + RIGHT_EYE_EDGES + RIGHT_IRIS_CENTER\n",
    "\n",
    "# Dataframe columns\n",
    "df_columns =  ['frame',\n",
    "'timestamp',\n",
    "'face_idx',\n",
    "'recognition_prediction',\n",
    "'recognition_distance',\n",
    "'attentive',\n",
    "'left_prediction',\n",
    "'left_score',\n",
    "'right_prediction',\n",
    "'right_score',\n",
    "'head_direction_prediction',\n",
    "'head_direction_score',\n",
    "'head_inclination_prediction',\n",
    "'head_inclination_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder_path = os.path.join(data_directory, \"frames\")\n",
    "image_paths = load_image_paths(image_folder_path)\n",
    "\n",
    "attention_df = pd.DataFrame(columns=df_columns)         \n",
    "\n",
    "for image_name, image_path in image_paths.items():\n",
    "    # Loading image\n",
    "    image = read_image(image_path)\n",
    "    timestamp = int(image_name.split('_ds')[1]) if '_ds' in image_name else np.nan\n",
    "    image_summary = image.copy()\n",
    "    # Splitting image\n",
    "    crops = split_image(image, n_split_w, n_split_h, 0.05, 0.05)\n",
    "    # Generating bboxes for each crop\n",
    "    bbox_crop_list = []\n",
    "    bbox_crop_list_absolute = []\n",
    "    for crop in crops:\n",
    "        coord_set = detect_face(crop.image)\n",
    "        bbox_crop_list.append(coord_set)\n",
    "        coord_set_absolute = reconstruct_coord(crop, coord_set)\n",
    "        bbox_crop_list_absolute.append(coord_set_absolute)\n",
    "    bbox_crop_list_absolute\n",
    "    # Eliminating duplicates bboxes\n",
    "    bbox_list = bbox_merge(bbox_crop_list_absolute, intersect_threshold=0.6)\n",
    "    # Drawing the unique bboxes on the original image\n",
    "    image_output = annotate_bboxes(image, bbox_list)\n",
    "    # Saving the image with its bboxes\n",
    "    bbox_path = os.path.join(data_directory, \"output_bbox\")\n",
    "    save_image(image_output, image_name + '.png', bbox_path)\n",
    "    \n",
    "    # Generating face crops\n",
    "    faces = crop_faces(image, bbox_list)\n",
    "    # Saving face crops\n",
    "    face_path = os.path.join(data_directory, \"face_crops\")\n",
    "    for face_idx, face in enumerate(faces):\n",
    "        face_name = image_name + f'_{face_idx}'\n",
    "        save_image(face, face_name + '.png', face_path)\n",
    "    \n",
    "    # Generating eye and iris landmarks\n",
    "    for face_idx, face in enumerate(faces):\n",
    "        face_name = image_name + f'_{face_idx}'\n",
    "        mp_landmarks = find_landmarks(face)\n",
    "        if mp_landmarks:                # Only run attention/ recognition if it detects a face\n",
    "            # Converting the Mediapipe landmark to a standard system of coordinates\n",
    "            landmark_list = convert_landmarks(face, mp_landmarks)\n",
    "            # Drawing the face mesh on the face\n",
    "            face_mesh = annotate_mesh(face, mp_landmarks)\n",
    "            # Saving face with complete mesh\n",
    "            mesh_path = os.path.join(data_directory, \"output_mesh\")\n",
    "            save_image(face_mesh, face_name + '.png', mesh_path)\n",
    "            \n",
    "            \n",
    "            # Detecting eye direction and attention\n",
    "            face_name = image_name + f'_{face_idx}'\n",
    "            eye_directions = detect_eye_directions(landmark_list, threshold = 0.63)\n",
    "            head_direction = detect_head_direction(landmark_list, left_threshold = 0.35, right_threshold = 1)\n",
    "            head_inclination = detect_head_inclination(landmark_list, down_threshold = 1.73, up_threshold = 0.8)\n",
    "            attention, attention_driver = is_attentive(eye_directions, head_direction, head_inclination)\n",
    "            # Drawing iris landmarks + annotating attention results on original image\n",
    "            prediction_left, score_left = eye_directions['left']\n",
    "            prediction_right, score_right = eye_directions['right']\n",
    "            prediction_head_direction, score_head_direction = head_direction\n",
    "            prediction_head_inclination, score_head_inclination = head_inclination\n",
    "            prediction_attention = 'attentive' if attention else 'inattentive'\n",
    "            face_attention = annotate_attention(face, landmark_list, \n",
    "                                                    prediction_left, score_left, \n",
    "                                                    prediction_right, score_right,\n",
    "                                                    prediction_head_direction, score_head_direction,\n",
    "                                                    prediction_head_inclination, score_head_inclination,\n",
    "                                                    prediction_attention)\n",
    "            # Saving attention image output\n",
    "            attention_path = os.path.join(data_directory, \"output_attention\")\n",
    "            save_image(face_attention, face_name + '.png', attention_path)\n",
    "        \n",
    "        \n",
    "            # Recognizing a face\n",
    "            face_name = image_name + f'_{face_idx}'\n",
    "            face_prediction = recognize_face(face, known_encodings)\n",
    "            # Annotating name and distance on the face image\n",
    "            prediction_recognition, distance_recognition = face_prediction\n",
    "            face_recognition = annotate_recognition(face, prediction_recognition, distance_recognition)\n",
    "            # Saving recognition image output\n",
    "            recognition_path = os.path.join(data_directory, \"output_recognition\")\n",
    "            save_image(face_recognition, face_name + '.png', recognition_path)\n",
    "            \n",
    "            \n",
    "            # Generating summary image\n",
    "            # Annotating with key info (bbox, attentiveness, recognition)\n",
    "            recognition = False if pd.isna(prediction_recognition) else True\n",
    "            bbox_face = bbox_list[face_idx]\n",
    "            image_summary = annotate_summary(image_summary, bbox_face, attention, attention_driver, recognition)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # Saving data in the dataframe\n",
    "            attention_df.loc[len(attention_df)] = [image_name,\n",
    "                                                    timestamp,\n",
    "                                                    face_idx,\n",
    "                                                    prediction_recognition,\n",
    "                                                    distance_recognition,\n",
    "                                                    attention,\n",
    "                                                    prediction_left,\n",
    "                                                    score_left,\n",
    "                                                    prediction_right,\n",
    "                                                    score_right,\n",
    "                                                    prediction_head_direction,\n",
    "                                                    score_head_direction,\n",
    "                                                    prediction_head_inclination,\n",
    "                                                    score_head_inclination]\n",
    "            \n",
    "    \n",
    "    # Saving summary image once all faces done\n",
    "    summary_path = os.path.join(data_directory, \"output_summary\")\n",
    "    save_image(image_summary, image_name + '.png', summary_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_df.to_csv(os.path.join(data_directory,'attention_output.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attention-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
